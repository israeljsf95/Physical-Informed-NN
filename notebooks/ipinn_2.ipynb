{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "from functools import partial\n",
    "import torch.nn as nn\n",
    "from torch.autograd import grad\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.io\n",
    "from scipy.interpolate import griddata\n",
    "import time\n",
    "from itertools import product, combinations\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from mpl_toolkits.mplot3d.art3d import Poly3DCollection\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "import matplotlib.gridspec as gridspec\n",
    "from tqdm.notebook import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0')\n",
    "#device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "#Loading the data\n",
    "data = scipy.io.loadmat(\"cylinder_nektar_wake.mat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of U: (5000, 2, 200)\n",
      "Shape of P: (5000, 200)\n",
      "Shape of t: (200, 1)\n",
      "Shape of X: (5000, 2)\n",
      "Shape of XX: (1, 1000000)\n",
      "Shape of YY: (1, 1000000)\n",
      "Shape of TT: (5000, 200)\n",
      "Shape of UU: (5000, 200)\n",
      "Shape of VV: (5000, 200)\n",
      "Shape of PP: (5000, 200)\n",
      "After flattening:\n",
      "Shape of U: (1000000, 1)\n",
      "Shape of U: (1000000, 1)\n",
      "Shape of U: (1000000, 1)\n",
      "Shape of U: (1000000, 1)\n",
      "Shape of U: (1000000, 1)\n",
      "Shape of U: (1000000, 1)\n"
     ]
    }
   ],
   "source": [
    "U_star = data['U_star'] # N x 2 x T\n",
    "P_star = data['p_star'] # N x T\n",
    "t_star = data['t'] # T x 1\n",
    "X_star = data['X_star'] # N x 2\n",
    "\n",
    "N = X_star.shape[0]\n",
    "T = t_star.shape[0]\n",
    "\n",
    "# Rearrange Data\n",
    "XX = np.tile(X_star[:,0], (1,T)) # N x T\n",
    "YY = np.tile(X_star[:,1], (1,T)) # N x T\n",
    "TT = np.tile(t_star, (1,N)).T # N x T\n",
    "\n",
    "UU = U_star[:,0,:] # N x T\n",
    "VV = U_star[:,1,:] # N x T\n",
    "PP = P_star # N x T\n",
    "\n",
    "x = XX.flatten()[:,None] # NT x 1\n",
    "y = YY.flatten()[:,None] # NT x 1\n",
    "t = TT.flatten()[:,None] # NT x 1\n",
    "\n",
    "u = UU.flatten()[:,None] # NT x 1\n",
    "v = VV.flatten()[:,None] # NT x 1\n",
    "p = PP.flatten()[:,None] # NT x 1\n",
    "\n",
    "print(f\"Shape of U: {U_star.shape}\")\n",
    "print(f\"Shape of P: {P_star.shape}\")\n",
    "print(f\"Shape of t: {t_star.shape}\")\n",
    "print(f\"Shape of X: {X_star.shape}\")\n",
    "print(f\"Shape of XX: {XX.shape}\")\n",
    "print(f\"Shape of YY: {YY.shape}\")\n",
    "print(f\"Shape of TT: {TT.shape}\")\n",
    "print(f\"Shape of UU: {UU.shape}\")\n",
    "print(f\"Shape of VV: {VV.shape}\")\n",
    "print(f\"Shape of PP: {PP.shape}\")\n",
    "print(\"After flattening:\")\n",
    "print(f\"Shape of U: {x.shape}\")\n",
    "print(f\"Shape of U: {y.shape}\")\n",
    "print(f\"Shape of U: {t.shape}\")\n",
    "print(f\"Shape of U: {u.shape}\")\n",
    "print(f\"Shape of U: {v.shape}\")\n",
    "print(f\"Shape of U: {p.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Sampling the training points\n",
    "N_train = 5000\n",
    "idx = np.random.choice(N*T, N_train, replace = False)\n",
    "x_train = x[idx,:]\n",
    "y_train = y[idx,:]\n",
    "t_train = t[idx,:]\n",
    "#t_train = np.sort(t_train, axis = 0)\n",
    "u_train = u[idx,:]\n",
    "v_train = v[idx,:]\n",
    "\n",
    "x_train = torch.from_numpy(x_train)\n",
    "x_train = x_train.to(torch.float32)\n",
    "x_train.requires_grad = True\n",
    "y_train = torch.from_numpy(y_train)\n",
    "y_train = y_train.to(torch.float32)\n",
    "y_train.requires_grad = True\n",
    "t_train = torch.from_numpy(t_train)\n",
    "t_train = t_train.to(torch.float32)\n",
    "t_train.requires_grad = True\n",
    "u_train = torch.from_numpy(u_train)\n",
    "u_train = u_train.to(torch.float32)\n",
    "u_train.requires_grad = True\n",
    "v_train = torch.from_numpy(v_train)\n",
    "v_train = v_train.to(torch.float32)\n",
    "v_train.requires_grad = True\n",
    "\n",
    "x_train = x_train.to(device)\n",
    "y_train = y_train.to(device)\n",
    "t_train = t_train.to(device)\n",
    "u_train = u_train.to(device)\n",
    "v_train = v_train.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(device(type='cuda', index=0),\n",
       " device(type='cuda', index=0),\n",
       " device(type='cuda', index=0),\n",
       " device(type='cuda', index=0),\n",
       " device(type='cuda', index=0),\n",
       " torch.float32,\n",
       " torch.float32)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.device, y_train.device, t_train.device, u_train.device, v_train.device, x_train.dtype, t_train.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FCNI(\n",
       "  (layers): Sequential(\n",
       "    (0): Linear(in_features=3, out_features=20, bias=True)\n",
       "    (1): Tanh()\n",
       "    (2): Linear(in_features=20, out_features=20, bias=True)\n",
       "    (3): Tanh()\n",
       "    (4): Linear(in_features=20, out_features=20, bias=True)\n",
       "    (5): Tanh()\n",
       "    (6): Linear(in_features=20, out_features=20, bias=True)\n",
       "    (7): Tanh()\n",
       "    (8): Linear(in_features=20, out_features=20, bias=True)\n",
       "    (9): Tanh()\n",
       "    (10): Linear(in_features=20, out_features=20, bias=True)\n",
       "    (11): Tanh()\n",
       "    (12): Linear(in_features=20, out_features=20, bias=True)\n",
       "    (13): Tanh()\n",
       "    (14): Linear(in_features=20, out_features=20, bias=True)\n",
       "    (15): Tanh()\n",
       "    (16): Linear(in_features=20, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class FCNI(nn.Module): # Forward\n",
    "\n",
    "\n",
    "    def __init__(self, n_input = 1, n_nodes = 5, n_hl = 3, n_output = 1, act = \"tanh\"):\n",
    "\n",
    "        super(FCNI, self).__init__()\n",
    "\n",
    "        if act == \"tanh\":\n",
    "            layers = [nn.Linear(n_input, n_nodes), nn.Tanh()]\n",
    "            for _ in range(n_hl - 1):\n",
    "                layers.append(nn.Linear(n_nodes, n_nodes))\n",
    "                layers.append(nn.Tanh())\n",
    "            layers.append(nn.Linear(n_nodes, n_output))\n",
    "        elif act == \"gelu\":\n",
    "            layers = [nn.Linear(n_input, n_nodes), nn.GELU()]\n",
    "            for _ in range(n_hl - 1):\n",
    "                layers.append(nn.Linear(n_nodes, n_nodes))\n",
    "                layers.append(nn.GELU())\n",
    "            layers.append(nn.Linear(n_nodes, n_output))\n",
    "        else:\n",
    "            layers = [nn.Linear(n_input, n_nodes), nn.ReLU()]\n",
    "            for _ in range(n_hl - 1):\n",
    "                layers.append(nn.Linear(n_nodes, n_nodes))\n",
    "                layers.append(nn.ReLU())\n",
    "            layers.append(nn.Linear(n_nodes, n_output))\n",
    "\n",
    "        self.layers = nn.Sequential(*layers)\n",
    "        self.lambda_1 = nn.Parameter(torch.tensor([0.0], requires_grad = True))\n",
    "        self.lambda_2 = nn.Parameter(torch.tensor([0.0], requires_grad = True))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "def init_weights(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        torch.nn.init.xavier_normal_(m.weight)\n",
    "        m.bias.data.fill_(0.0)\n",
    "\n",
    "model_i = FCNI(n_input = 3, n_nodes = 20, n_hl = 8, n_output = 2, act = 'tanh')\n",
    "model_i.apply(init_weights)\n",
    "model_i.to(device)\n",
    "\n",
    "opt = torch.optim.Adam(model_i.parameters(), lr = 1e-3)\n",
    "iter = 0\n",
    "model_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def compute_partials(model, x, y, t):\n",
    "\n",
    "    # x = torch.squeeze(x)\n",
    "    # y = torch.squeeze(y)\n",
    "    # t = torch.squeeze(t)\n",
    "    # d_train = torch.concat((x.view(-1, 1), y.view(-1, 1), t.view(-1, 1)), axis = 1)\n",
    "    # d_train = torch.unsqueeze(d_train, 0)\n",
    "    # pred = model(d_train)\n",
    "    # pred = torch.squeeze(pred)\n",
    "    # psi = pred[: , 0]\n",
    "    # p = pred[:, 1]\n",
    "    # psi = torch.squeeze(psi)\n",
    "    # p = torch.squeeze(p)\n",
    "    # u =  grad(psi, y, torch.ones_like(y), create_graph = True, retain_graph = True)[0]\n",
    "    # v = -1.*grad(psi, x, torch.ones_like(x), create_graph = True, retain_graph = True)[0]\n",
    "\n",
    "\n",
    "    # #computing physics for u\n",
    "    # u_t = grad(u, t, torch.ones_like(u), create_graph = True, retain_graph = True)[0]\n",
    "    # u_x = grad(u, x, torch.ones_like(u), create_graph = True, retain_graph = True)[0]\n",
    "    # u_y = grad(u, y, torch.ones_like(u), create_graph = True, retain_graph = True)[0]\n",
    "    # u_xx = grad(u_x, x, torch.ones_like(u), create_graph = True, retain_graph = True)[0]\n",
    "    # u_yy = grad(u_y, y, torch.ones_like(u), create_graph = True, retain_graph = True)[0]\n",
    "\n",
    "    # #computing physics for v\n",
    "    # v_t = grad(v, t, torch.ones_like(v), create_graph = True, retain_graph = True)[0]\n",
    "    # v_x = grad(v, x, torch.ones_like(v), create_graph = True, retain_graph = True)[0]\n",
    "    # v_y = grad(v, y, torch.ones_like(v), create_graph = True, retain_graph = True)[0]\n",
    "    # v_xx = grad(v_x, x, torch.ones_like(v), create_graph = True, retain_graph = True)[0]\n",
    "    # v_yy = grad(v_y, y, torch.ones_like(v), create_graph = True, retain_graph = True)[0]\n",
    "\n",
    "    # #computing physics for p\n",
    "    # p_x = grad(p, x, torch.ones_like(y), create_graph = True, retain_graph = True)[0]\n",
    "    # p_y = grad(p, y, torch.ones_like(y), create_graph = True, retain_graph = True)[0]\n",
    "\n",
    "    # f_u = u_t + model.lambda_1*(u*u_x + v*u_y) + p_x - model.lambda_2*(u_xx + u_yy)\n",
    "    # f_v = v_t + model.lambda_1*(u*v_x + v*v_y) + p_y - model.lambda_2*(v_xx + v_yy)\n",
    "    res = model(torch.hstack((x, y, t)))\n",
    "    psi, p = res[:, 0:1], res[:, 1:2]\n",
    "\n",
    "    u = torch.autograd.grad(psi, y, grad_outputs=torch.ones_like(psi), create_graph=True)[0] #retain_graph=True,\n",
    "    v = -1.*torch.autograd.grad(psi, x, grad_outputs=torch.ones_like(psi), create_graph=True)[0]\n",
    "\n",
    "    u_x = torch.autograd.grad(u, x, grad_outputs=torch.ones_like(u), create_graph=True)[0]\n",
    "    u_xx = torch.autograd.grad(u_x, x, grad_outputs=torch.ones_like(u_x), create_graph=True)[0]\n",
    "    u_y = torch.autograd.grad(u, y, grad_outputs=torch.ones_like(u), create_graph=True)[0]\n",
    "    u_yy = torch.autograd.grad(u_y, y, grad_outputs=torch.ones_like(u_y), create_graph=True)[0]\n",
    "    u_t = torch.autograd.grad(u, t, grad_outputs=torch.ones_like(u), create_graph=True)[0]\n",
    "\n",
    "    v_x = torch.autograd.grad(v, x, grad_outputs=torch.ones_like(v), create_graph=True)[0]\n",
    "    v_xx = torch.autograd.grad(v_x, x, grad_outputs=torch.ones_like(v_x), create_graph=True)[0]\n",
    "    v_y = torch.autograd.grad(v, y, grad_outputs=torch.ones_like(v), create_graph=True)[0]\n",
    "    v_yy = torch.autograd.grad(v_y, y, grad_outputs=torch.ones_like(v_y), create_graph=True)[0]\n",
    "    v_t = torch.autograd.grad(v, t, grad_outputs=torch.ones_like(v), create_graph=True)[0]\n",
    "\n",
    "    p_x = torch.autograd.grad(p, x, grad_outputs=torch.ones_like(p), create_graph=True)[0]\n",
    "    p_y = torch.autograd.grad(p, y, grad_outputs=torch.ones_like(p), create_graph=True)[0]\n",
    "\n",
    "    f_u = u_t + model.lambda_1*(u * u_x + v * u_y) + p_x - model.lambda_2 * (u_xx + u_yy)\n",
    "    f_v = v_t + model.lambda_1*(u * v_x + v * v_y) + p_y - model.lambda_2 * (v_xx + v_yy)\n",
    "\n",
    "    #returning as column tensors\n",
    "    return u.view(-1, 1), v.view(-1, 1), p.view(-1, 1), f_u.view(-1, 1), f_v.view(-1, 1)\n",
    "\n",
    "\n",
    "def loss_complete(model, x, y, t, u_ori, v_ori):\n",
    "\n",
    "    mse = nn.MSELoss()\n",
    "    u_pred, v_pred, p_pred, f_u, f_v = compute_partials(model, x, y, t)\n",
    "    aux = torch.zeros((x.shape[0], 1)).to(device)\n",
    "    l1 = mse(u_ori, u_pred)\n",
    "    l2 = mse(v_ori, v_pred)\n",
    "    l3 = mse(f_u , aux)\n",
    "    l4 = mse(f_v, aux)\n",
    "    loss = (l1 + l2 + (l3 + l4))\n",
    "    return loss\n",
    "\n",
    "\n",
    "def tr_step(model, opt, x, y, t, u_ori, v_ori, lrlambdas = 1e-2):\n",
    "\n",
    "\n",
    "    opt.zero_grad()\n",
    "\n",
    "    l = loss_complete(model, x, y, t, u_ori, v_ori)\n",
    "    l.backward()\n",
    "    # model.lambda_1.data.sub_(lrlambdas * model.lambda_1.grad)\n",
    "    # model.lambda_2.data.sub_(lrlambdas * model.lambda_2.grad)\n",
    "    # model.lambda_1.grad.zero_()\n",
    "    # model.lambda_2.grad.zero_()\n",
    "\n",
    "    opt.step()\n",
    "\n",
    "    return l.item()\n",
    "\n",
    "def closure(model, opt, x, y, t, u_ori, v_ori, lrlambdas = 3e-3):\n",
    "    \"\"\"\n",
    "    The closure function to use L-BFGS optimization method.\n",
    "    \"\"\"\n",
    "    opt.zero_grad()\n",
    "    # evaluating the MSE for the PDE\n",
    "    loss = loss_complete(model, x, y, t, u_ori, v_ori)\n",
    "    loss.backward()\n",
    "    # model.lambda_1.data.sub_(lrlambdas * model.lambda_1.grad)\n",
    "    # model.lambda_2.data.sub_(lrlambdas * model.lambda_2.grad)\n",
    "    # model.lambda_1.grad.zero_()\n",
    "    # model.lambda_2.grad.zero_()\n",
    "\n",
    "    tr_loss.append(loss.item())\n",
    "    global iter\n",
    "    iter += 1\n",
    "    if iter % 20 == 0:\n",
    "        print(f\"iteration: {iter}  loss: {loss.item():.6f}\")\n",
    "        print(f\"lambda1 = {1.00}   || lambda_hat_1 = {model.lambda_1.item():.6f}\")\n",
    "        print(f\"lambda2 = {0.01}   || lambda_hat_2 = {model.lambda_2.item():.6f}\")\n",
    "\n",
    "    if iter % 30==0:\n",
    "        torch.save(model.state_dict(), f'models_trained/model_LBFGS_{iter}.pt')\n",
    "    return loss\n",
    "\n",
    "def tr_lbfgs(model, x, y, t, u_ori, v_ori):\n",
    "    # Initialize the optimizer\n",
    "    ls = [None, \"strong_wolfe\"]\n",
    "    optimizer = torch.optim.LBFGS(model.parameters(),\n",
    "                                  lr = 1,\n",
    "                                  max_iter = 200000,\n",
    "                                  max_eval = 50000,\n",
    "                                  history_size = 50,\n",
    "                                  tolerance_grad = 1e-05,\n",
    "                                  tolerance_change = .5 * np.finfo(float).eps,\n",
    "                                  line_search_fn = ls[1])\n",
    "\n",
    "    # the optimizer.step requires the closure function to be a callable function without inputs\n",
    "    # therefore we need to define a partial function and pass it to the optimizer\n",
    "    closure_fn = partial(closure, model, optimizer, x, y, t, u_ori, v_ori)\n",
    "    optimizer.step(closure_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 20  loss: 0.188374\n",
      "lambda1 = 1.0   || lambda_hat_1 = 0.034982\n",
      "lambda2 = 0.01   || lambda_hat_2 = -0.002702\n",
      "iteration: 40  loss: 0.155113\n",
      "lambda1 = 1.0   || lambda_hat_1 = 0.081731\n",
      "lambda2 = 0.01   || lambda_hat_2 = -0.000476\n",
      "iteration: 60  loss: 0.153212\n",
      "lambda1 = 1.0   || lambda_hat_1 = 0.137316\n",
      "lambda2 = 0.01   || lambda_hat_2 = 0.049825\n",
      "iteration: 80  loss: 0.152903\n",
      "lambda1 = 1.0   || lambda_hat_1 = 0.168323\n",
      "lambda2 = 0.01   || lambda_hat_2 = 0.040999\n",
      "iteration: 100  loss: 0.152742\n",
      "lambda1 = 1.0   || lambda_hat_1 = 0.171194\n",
      "lambda2 = 0.01   || lambda_hat_2 = 0.006046\n",
      "iteration: 120  loss: 0.152628\n",
      "lambda1 = 1.0   || lambda_hat_1 = 0.153889\n",
      "lambda2 = 0.01   || lambda_hat_2 = -0.037654\n",
      "iteration: 140  loss: 0.152512\n",
      "lambda1 = 1.0   || lambda_hat_1 = 0.142928\n",
      "lambda2 = 0.01   || lambda_hat_2 = -0.048972\n",
      "iteration: 160  loss: 0.152445\n",
      "lambda1 = 1.0   || lambda_hat_1 = 0.144420\n",
      "lambda2 = 0.01   || lambda_hat_2 = -0.048392\n",
      "iteration: 180  loss: 0.152398\n",
      "lambda1 = 1.0   || lambda_hat_1 = 0.161226\n",
      "lambda2 = 0.01   || lambda_hat_2 = -0.050589\n",
      "iteration: 200  loss: 0.152327\n",
      "lambda1 = 1.0   || lambda_hat_1 = 0.222315\n",
      "lambda2 = 0.01   || lambda_hat_2 = -0.036668\n",
      "iteration: 220  loss: 0.152256\n",
      "lambda1 = 1.0   || lambda_hat_1 = 0.309459\n",
      "lambda2 = 0.01   || lambda_hat_2 = -0.017727\n",
      "iteration: 240  loss: 0.152189\n",
      "lambda1 = 1.0   || lambda_hat_1 = 0.315421\n",
      "lambda2 = 0.01   || lambda_hat_2 = -0.026494\n",
      "iteration: 260  loss: 0.152129\n",
      "lambda1 = 1.0   || lambda_hat_1 = 0.334527\n",
      "lambda2 = 0.01   || lambda_hat_2 = -0.032595\n",
      "iteration: 280  loss: 0.152059\n",
      "lambda1 = 1.0   || lambda_hat_1 = 0.354012\n",
      "lambda2 = 0.01   || lambda_hat_2 = -0.054422\n",
      "iteration: 300  loss: 0.151974\n",
      "lambda1 = 1.0   || lambda_hat_1 = 0.321990\n",
      "lambda2 = 0.01   || lambda_hat_2 = -0.076540\n",
      "iteration: 320  loss: 0.151905\n",
      "lambda1 = 1.0   || lambda_hat_1 = 0.331044\n",
      "lambda2 = 0.01   || lambda_hat_2 = -0.066455\n",
      "iteration: 340  loss: 0.151861\n",
      "lambda1 = 1.0   || lambda_hat_1 = 0.316055\n",
      "lambda2 = 0.01   || lambda_hat_2 = -0.065962\n",
      "iteration: 360  loss: 0.151814\n",
      "lambda1 = 1.0   || lambda_hat_1 = 0.292909\n",
      "lambda2 = 0.01   || lambda_hat_2 = -0.066695\n",
      "iteration: 380  loss: 0.151770\n",
      "lambda1 = 1.0   || lambda_hat_1 = 0.283663\n",
      "lambda2 = 0.01   || lambda_hat_2 = -0.061626\n",
      "iteration: 400  loss: 0.151715\n",
      "lambda1 = 1.0   || lambda_hat_1 = 0.290046\n",
      "lambda2 = 0.01   || lambda_hat_2 = -0.055195\n",
      "iteration: 420  loss: 0.151679\n",
      "lambda1 = 1.0   || lambda_hat_1 = 0.296900\n",
      "lambda2 = 0.01   || lambda_hat_2 = -0.062127\n",
      "iteration: 440  loss: 0.151638\n",
      "lambda1 = 1.0   || lambda_hat_1 = 0.295482\n",
      "lambda2 = 0.01   || lambda_hat_2 = -0.066954\n",
      "iteration: 460  loss: 0.151608\n",
      "lambda1 = 1.0   || lambda_hat_1 = 0.318943\n",
      "lambda2 = 0.01   || lambda_hat_2 = -0.062902\n",
      "iteration: 480  loss: 0.151577\n",
      "lambda1 = 1.0   || lambda_hat_1 = 0.325583\n",
      "lambda2 = 0.01   || lambda_hat_2 = -0.060369\n",
      "iteration: 500  loss: 0.151549\n",
      "lambda1 = 1.0   || lambda_hat_1 = 0.335296\n",
      "lambda2 = 0.01   || lambda_hat_2 = -0.059291\n",
      "iteration: 520  loss: 0.151525\n",
      "lambda1 = 1.0   || lambda_hat_1 = 0.345047\n",
      "lambda2 = 0.01   || lambda_hat_2 = -0.056177\n",
      "iteration: 540  loss: 0.151499\n",
      "lambda1 = 1.0   || lambda_hat_1 = 0.348713\n",
      "lambda2 = 0.01   || lambda_hat_2 = -0.059391\n",
      "iteration: 560  loss: 0.151486\n",
      "lambda1 = 1.0   || lambda_hat_1 = 0.355770\n",
      "lambda2 = 0.01   || lambda_hat_2 = -0.052559\n",
      "iteration: 580  loss: 0.151468\n",
      "lambda1 = 1.0   || lambda_hat_1 = 0.351279\n",
      "lambda2 = 0.01   || lambda_hat_2 = -0.051060\n",
      "iteration: 600  loss: 0.151453\n",
      "lambda1 = 1.0   || lambda_hat_1 = 0.356223\n",
      "lambda2 = 0.01   || lambda_hat_2 = -0.047733\n",
      "iteration: 620  loss: 0.151430\n",
      "lambda1 = 1.0   || lambda_hat_1 = 0.363734\n",
      "lambda2 = 0.01   || lambda_hat_2 = -0.044973\n",
      "iteration: 640  loss: 0.151414\n",
      "lambda1 = 1.0   || lambda_hat_1 = 0.370154\n",
      "lambda2 = 0.01   || lambda_hat_2 = -0.044848\n",
      "iteration: 660  loss: 0.151394\n",
      "lambda1 = 1.0   || lambda_hat_1 = 0.379770\n",
      "lambda2 = 0.01   || lambda_hat_2 = -0.049198\n",
      "iteration: 680  loss: 0.151369\n",
      "lambda1 = 1.0   || lambda_hat_1 = 0.388002\n",
      "lambda2 = 0.01   || lambda_hat_2 = -0.049791\n",
      "iteration: 700  loss: 0.151352\n",
      "lambda1 = 1.0   || lambda_hat_1 = 0.391671\n",
      "lambda2 = 0.01   || lambda_hat_2 = -0.045662\n",
      "iteration: 720  loss: 0.151332\n",
      "lambda1 = 1.0   || lambda_hat_1 = 0.389217\n",
      "lambda2 = 0.01   || lambda_hat_2 = -0.041084\n",
      "iteration: 740  loss: 0.151312\n",
      "lambda1 = 1.0   || lambda_hat_1 = 0.386409\n",
      "lambda2 = 0.01   || lambda_hat_2 = -0.039225\n",
      "iteration: 760  loss: 0.151294\n",
      "lambda1 = 1.0   || lambda_hat_1 = 0.377741\n",
      "lambda2 = 0.01   || lambda_hat_2 = -0.038936\n",
      "iteration: 780  loss: 0.151282\n",
      "lambda1 = 1.0   || lambda_hat_1 = 0.377169\n",
      "lambda2 = 0.01   || lambda_hat_2 = -0.042497\n",
      "iteration: 800  loss: 0.151274\n",
      "lambda1 = 1.0   || lambda_hat_1 = 0.377307\n",
      "lambda2 = 0.01   || lambda_hat_2 = -0.045894\n",
      "iteration: 820  loss: 0.151262\n",
      "lambda1 = 1.0   || lambda_hat_1 = 0.380914\n",
      "lambda2 = 0.01   || lambda_hat_2 = -0.045681\n",
      "iteration: 840  loss: 0.151243\n",
      "lambda1 = 1.0   || lambda_hat_1 = 0.383633\n",
      "lambda2 = 0.01   || lambda_hat_2 = -0.042063\n",
      "iteration: 860  loss: 0.151233\n",
      "lambda1 = 1.0   || lambda_hat_1 = 0.386728\n",
      "lambda2 = 0.01   || lambda_hat_2 = -0.042136\n",
      "iteration: 880  loss: 0.151223\n",
      "lambda1 = 1.0   || lambda_hat_1 = 0.384531\n",
      "lambda2 = 0.01   || lambda_hat_2 = -0.045013\n",
      "iteration: 900  loss: 0.151211\n",
      "lambda1 = 1.0   || lambda_hat_1 = 0.380441\n",
      "lambda2 = 0.01   || lambda_hat_2 = -0.046887\n",
      "iteration: 920  loss: 0.151198\n",
      "lambda1 = 1.0   || lambda_hat_1 = 0.374206\n",
      "lambda2 = 0.01   || lambda_hat_2 = -0.048187\n",
      "iteration: 940  loss: 0.151184\n",
      "lambda1 = 1.0   || lambda_hat_1 = 0.369048\n",
      "lambda2 = 0.01   || lambda_hat_2 = -0.046702\n",
      "iteration: 960  loss: 0.151176\n",
      "lambda1 = 1.0   || lambda_hat_1 = 0.366792\n",
      "lambda2 = 0.01   || lambda_hat_2 = -0.043645\n",
      "iteration: 980  loss: 0.151167\n",
      "lambda1 = 1.0   || lambda_hat_1 = 0.359476\n",
      "lambda2 = 0.01   || lambda_hat_2 = -0.041075\n",
      "iteration: 1000  loss: 0.151157\n",
      "lambda1 = 1.0   || lambda_hat_1 = 0.350338\n",
      "lambda2 = 0.01   || lambda_hat_2 = -0.042321\n",
      "iteration: 1020  loss: 0.151138\n",
      "lambda1 = 1.0   || lambda_hat_1 = 0.342377\n",
      "lambda2 = 0.01   || lambda_hat_2 = -0.039143\n",
      "iteration: 1040  loss: 0.151127\n",
      "lambda1 = 1.0   || lambda_hat_1 = 0.341588\n",
      "lambda2 = 0.01   || lambda_hat_2 = -0.036306\n",
      "iteration: 1060  loss: 0.151116\n",
      "lambda1 = 1.0   || lambda_hat_1 = 0.337280\n",
      "lambda2 = 0.01   || lambda_hat_2 = -0.038135\n",
      "iteration: 1080  loss: 0.151100\n",
      "lambda1 = 1.0   || lambda_hat_1 = 0.339684\n",
      "lambda2 = 0.01   || lambda_hat_2 = -0.036677\n",
      "iteration: 1100  loss: 0.151085\n",
      "lambda1 = 1.0   || lambda_hat_1 = 0.336783\n",
      "lambda2 = 0.01   || lambda_hat_2 = -0.038429\n",
      "iteration: 1120  loss: 0.151076\n",
      "lambda1 = 1.0   || lambda_hat_1 = 0.336371\n",
      "lambda2 = 0.01   || lambda_hat_2 = -0.037652\n",
      "iteration: 1140  loss: 0.151058\n",
      "lambda1 = 1.0   || lambda_hat_1 = 0.338817\n",
      "lambda2 = 0.01   || lambda_hat_2 = -0.035784\n",
      "iteration: 1160  loss: 0.151052\n",
      "lambda1 = 1.0   || lambda_hat_1 = 0.337802\n",
      "lambda2 = 0.01   || lambda_hat_2 = -0.037015\n",
      "iteration: 1180  loss: 0.151034\n",
      "lambda1 = 1.0   || lambda_hat_1 = 0.337212\n",
      "lambda2 = 0.01   || lambda_hat_2 = -0.031101\n",
      "iteration: 1200  loss: 0.151024\n",
      "lambda1 = 1.0   || lambda_hat_1 = 0.333779\n",
      "lambda2 = 0.01   || lambda_hat_2 = -0.032174\n",
      "iteration: 1220  loss: 0.151011\n",
      "lambda1 = 1.0   || lambda_hat_1 = 0.328927\n",
      "lambda2 = 0.01   || lambda_hat_2 = -0.032707\n",
      "iteration: 1240  loss: 0.150996\n",
      "lambda1 = 1.0   || lambda_hat_1 = 0.324967\n",
      "lambda2 = 0.01   || lambda_hat_2 = -0.029324\n",
      "iteration: 1260  loss: 0.150981\n",
      "lambda1 = 1.0   || lambda_hat_1 = 0.319099\n",
      "lambda2 = 0.01   || lambda_hat_2 = -0.024684\n",
      "iteration: 1280  loss: 0.150971\n",
      "lambda1 = 1.0   || lambda_hat_1 = 0.318080\n",
      "lambda2 = 0.01   || lambda_hat_2 = -0.024350\n",
      "iteration: 1300  loss: 0.150950\n",
      "lambda1 = 1.0   || lambda_hat_1 = 0.319866\n",
      "lambda2 = 0.01   || lambda_hat_2 = -0.028407\n",
      "iteration: 1320  loss: 0.150930\n",
      "lambda1 = 1.0   || lambda_hat_1 = 0.317938\n",
      "lambda2 = 0.01   || lambda_hat_2 = -0.025376\n",
      "iteration: 1340  loss: 0.150912\n",
      "lambda1 = 1.0   || lambda_hat_1 = 0.322368\n",
      "lambda2 = 0.01   || lambda_hat_2 = -0.025889\n",
      "iteration: 1360  loss: 0.150890\n",
      "lambda1 = 1.0   || lambda_hat_1 = 0.327473\n",
      "lambda2 = 0.01   || lambda_hat_2 = -0.026348\n",
      "iteration: 1380  loss: 0.150871\n",
      "lambda1 = 1.0   || lambda_hat_1 = 0.325834\n",
      "lambda2 = 0.01   || lambda_hat_2 = -0.027286\n",
      "iteration: 1400  loss: 0.150844\n",
      "lambda1 = 1.0   || lambda_hat_1 = 0.328646\n",
      "lambda2 = 0.01   || lambda_hat_2 = -0.025430\n",
      "iteration: 1420  loss: 0.150834\n",
      "lambda1 = 1.0   || lambda_hat_1 = 0.327370\n",
      "lambda2 = 0.01   || lambda_hat_2 = -0.024210\n",
      "iteration: 1440  loss: 0.150818\n",
      "lambda1 = 1.0   || lambda_hat_1 = 0.328154\n",
      "lambda2 = 0.01   || lambda_hat_2 = -0.024560\n",
      "iteration: 1460  loss: 0.150794\n",
      "lambda1 = 1.0   || lambda_hat_1 = 0.331382\n",
      "lambda2 = 0.01   || lambda_hat_2 = -0.026055\n",
      "iteration: 1480  loss: 0.150779\n",
      "lambda1 = 1.0   || lambda_hat_1 = 0.331253\n",
      "lambda2 = 0.01   || lambda_hat_2 = -0.022282\n",
      "iteration: 1500  loss: 0.150768\n",
      "lambda1 = 1.0   || lambda_hat_1 = 0.330976\n",
      "lambda2 = 0.01   || lambda_hat_2 = -0.021941\n",
      "iteration: 1520  loss: 0.150756\n",
      "lambda1 = 1.0   || lambda_hat_1 = 0.332045\n",
      "lambda2 = 0.01   || lambda_hat_2 = -0.023224\n",
      "iteration: 1540  loss: 0.150747\n",
      "lambda1 = 1.0   || lambda_hat_1 = 0.330762\n",
      "lambda2 = 0.01   || lambda_hat_2 = -0.025951\n",
      "iteration: 1560  loss: 0.150738\n",
      "lambda1 = 1.0   || lambda_hat_1 = 0.329920\n",
      "lambda2 = 0.01   || lambda_hat_2 = -0.026565\n",
      "iteration: 1580  loss: 0.150724\n",
      "lambda1 = 1.0   || lambda_hat_1 = 0.328035\n",
      "lambda2 = 0.01   || lambda_hat_2 = -0.024787\n",
      "iteration: 1600  loss: 0.150714\n",
      "lambda1 = 1.0   || lambda_hat_1 = 0.326812\n",
      "lambda2 = 0.01   || lambda_hat_2 = -0.024566\n",
      "iteration: 1620  loss: 0.150702\n",
      "lambda1 = 1.0   || lambda_hat_1 = 0.329286\n",
      "lambda2 = 0.01   || lambda_hat_2 = -0.023606\n",
      "iteration: 1640  loss: 0.150693\n",
      "lambda1 = 1.0   || lambda_hat_1 = 0.327656\n",
      "lambda2 = 0.01   || lambda_hat_2 = -0.023395\n",
      "iteration: 1660  loss: 0.150677\n",
      "lambda1 = 1.0   || lambda_hat_1 = 0.327048\n",
      "lambda2 = 0.01   || lambda_hat_2 = -0.023812\n",
      "iteration: 1680  loss: 0.150665\n",
      "lambda1 = 1.0   || lambda_hat_1 = 0.328638\n",
      "lambda2 = 0.01   || lambda_hat_2 = -0.023767\n",
      "iteration: 1700  loss: 0.150648\n",
      "lambda1 = 1.0   || lambda_hat_1 = 0.327342\n",
      "lambda2 = 0.01   || lambda_hat_2 = -0.020688\n",
      "iteration: 1720  loss: 0.150635\n",
      "lambda1 = 1.0   || lambda_hat_1 = 0.328210\n",
      "lambda2 = 0.01   || lambda_hat_2 = -0.021621\n",
      "iteration: 1740  loss: 0.150628\n",
      "lambda1 = 1.0   || lambda_hat_1 = 0.329444\n",
      "lambda2 = 0.01   || lambda_hat_2 = -0.021947\n",
      "iteration: 1760  loss: 0.150623\n",
      "lambda1 = 1.0   || lambda_hat_1 = 0.329576\n",
      "lambda2 = 0.01   || lambda_hat_2 = -0.020794\n",
      "iteration: 1780  loss: 0.150615\n",
      "lambda1 = 1.0   || lambda_hat_1 = 0.330665\n",
      "lambda2 = 0.01   || lambda_hat_2 = -0.021069\n",
      "iteration: 1800  loss: 0.150602\n",
      "lambda1 = 1.0   || lambda_hat_1 = 0.331674\n",
      "lambda2 = 0.01   || lambda_hat_2 = -0.021171\n",
      "iteration: 1820  loss: 0.150596\n",
      "lambda1 = 1.0   || lambda_hat_1 = 0.331172\n",
      "lambda2 = 0.01   || lambda_hat_2 = -0.020759\n",
      "iteration: 1840  loss: 0.150586\n",
      "lambda1 = 1.0   || lambda_hat_1 = 0.329586\n",
      "lambda2 = 0.01   || lambda_hat_2 = -0.020388\n",
      "iteration: 1860  loss: 0.150578\n",
      "lambda1 = 1.0   || lambda_hat_1 = 0.328137\n",
      "lambda2 = 0.01   || lambda_hat_2 = -0.018045\n",
      "iteration: 1880  loss: 0.150570\n",
      "lambda1 = 1.0   || lambda_hat_1 = 0.327111\n",
      "lambda2 = 0.01   || lambda_hat_2 = -0.016888\n",
      "iteration: 1900  loss: 0.150563\n",
      "lambda1 = 1.0   || lambda_hat_1 = 0.326775\n",
      "lambda2 = 0.01   || lambda_hat_2 = -0.017249\n",
      "iteration: 1920  loss: 0.150555\n",
      "lambda1 = 1.0   || lambda_hat_1 = 0.325272\n",
      "lambda2 = 0.01   || lambda_hat_2 = -0.018178\n",
      "iteration: 1940  loss: 0.150546\n",
      "lambda1 = 1.0   || lambda_hat_1 = 0.323932\n",
      "lambda2 = 0.01   || lambda_hat_2 = -0.017593\n",
      "iteration: 1960  loss: 0.150540\n",
      "lambda1 = 1.0   || lambda_hat_1 = 0.321481\n",
      "lambda2 = 0.01   || lambda_hat_2 = -0.017968\n",
      "iteration: 1980  loss: 0.150529\n",
      "lambda1 = 1.0   || lambda_hat_1 = 0.317201\n",
      "lambda2 = 0.01   || lambda_hat_2 = -0.016605\n",
      "iteration: 2000  loss: 0.150521\n",
      "lambda1 = 1.0   || lambda_hat_1 = 0.315244\n",
      "lambda2 = 0.01   || lambda_hat_2 = -0.017203\n",
      "iteration: 2020  loss: 0.150510\n",
      "lambda1 = 1.0   || lambda_hat_1 = 0.311266\n",
      "lambda2 = 0.01   || lambda_hat_2 = -0.017214\n",
      "iteration: 2040  loss: 0.150503\n",
      "lambda1 = 1.0   || lambda_hat_1 = 0.308948\n",
      "lambda2 = 0.01   || lambda_hat_2 = -0.017706\n",
      "iteration: 2060  loss: 0.150486\n",
      "lambda1 = 1.0   || lambda_hat_1 = 0.302886\n",
      "lambda2 = 0.01   || lambda_hat_2 = -0.017707\n",
      "iteration: 2080  loss: 0.150472\n",
      "lambda1 = 1.0   || lambda_hat_1 = 0.294172\n",
      "lambda2 = 0.01   || lambda_hat_2 = -0.017562\n",
      "iteration: 2100  loss: 0.150459\n",
      "lambda1 = 1.0   || lambda_hat_1 = 0.289357\n",
      "lambda2 = 0.01   || lambda_hat_2 = -0.016820\n",
      "iteration: 2120  loss: 0.150447\n",
      "lambda1 = 1.0   || lambda_hat_1 = 0.286063\n",
      "lambda2 = 0.01   || lambda_hat_2 = -0.015966\n",
      "iteration: 2140  loss: 0.150439\n",
      "lambda1 = 1.0   || lambda_hat_1 = 0.281831\n",
      "lambda2 = 0.01   || lambda_hat_2 = -0.016625\n",
      "iteration: 2160  loss: 0.150434\n",
      "lambda1 = 1.0   || lambda_hat_1 = 0.281028\n",
      "lambda2 = 0.01   || lambda_hat_2 = -0.016659\n",
      "iteration: 2180  loss: 0.150426\n",
      "lambda1 = 1.0   || lambda_hat_1 = 0.280434\n",
      "lambda2 = 0.01   || lambda_hat_2 = -0.018219\n",
      "iteration: 2200  loss: 0.150420\n",
      "lambda1 = 1.0   || lambda_hat_1 = 0.281174\n",
      "lambda2 = 0.01   || lambda_hat_2 = -0.017328\n",
      "iteration: 2220  loss: 0.150412\n",
      "lambda1 = 1.0   || lambda_hat_1 = 0.279906\n",
      "lambda2 = 0.01   || lambda_hat_2 = -0.016404\n",
      "iteration: 2240  loss: 0.150405\n",
      "lambda1 = 1.0   || lambda_hat_1 = 0.283311\n",
      "lambda2 = 0.01   || lambda_hat_2 = -0.016289\n",
      "iteration: 2260  loss: 0.150397\n",
      "lambda1 = 1.0   || lambda_hat_1 = 0.285686\n",
      "lambda2 = 0.01   || lambda_hat_2 = -0.016325\n",
      "iteration: 2280  loss: 0.150393\n",
      "lambda1 = 1.0   || lambda_hat_1 = 0.287124\n",
      "lambda2 = 0.01   || lambda_hat_2 = -0.016393\n",
      "iteration: 2300  loss: 0.150388\n",
      "lambda1 = 1.0   || lambda_hat_1 = 0.286445\n",
      "lambda2 = 0.01   || lambda_hat_2 = -0.016012\n",
      "iteration: 2320  loss: 0.150381\n",
      "lambda1 = 1.0   || lambda_hat_1 = 0.285653\n",
      "lambda2 = 0.01   || lambda_hat_2 = -0.015119\n",
      "iteration: 2340  loss: 0.150373\n",
      "lambda1 = 1.0   || lambda_hat_1 = 0.285271\n",
      "lambda2 = 0.01   || lambda_hat_2 = -0.015769\n",
      "iteration: 2360  loss: 0.150366\n",
      "lambda1 = 1.0   || lambda_hat_1 = 0.284283\n",
      "lambda2 = 0.01   || lambda_hat_2 = -0.015882\n",
      "iteration: 2380  loss: 0.150360\n",
      "lambda1 = 1.0   || lambda_hat_1 = 0.284320\n",
      "lambda2 = 0.01   || lambda_hat_2 = -0.015495\n",
      "iteration: 2400  loss: 0.150356\n",
      "lambda1 = 1.0   || lambda_hat_1 = 0.281965\n",
      "lambda2 = 0.01   || lambda_hat_2 = -0.014538\n",
      "iteration: 2420  loss: 0.150350\n",
      "lambda1 = 1.0   || lambda_hat_1 = 0.279508\n",
      "lambda2 = 0.01   || lambda_hat_2 = -0.015205\n",
      "iteration: 2440  loss: 0.150344\n",
      "lambda1 = 1.0   || lambda_hat_1 = 0.277231\n",
      "lambda2 = 0.01   || lambda_hat_2 = -0.016599\n",
      "iteration: 2460  loss: 0.150336\n",
      "lambda1 = 1.0   || lambda_hat_1 = 0.274154\n",
      "lambda2 = 0.01   || lambda_hat_2 = -0.017207\n",
      "iteration: 2480  loss: 0.150331\n",
      "lambda1 = 1.0   || lambda_hat_1 = 0.272567\n",
      "lambda2 = 0.01   || lambda_hat_2 = -0.015882\n",
      "iteration: 2500  loss: 0.150326\n",
      "lambda1 = 1.0   || lambda_hat_1 = 0.270771\n",
      "lambda2 = 0.01   || lambda_hat_2 = -0.015054\n",
      "iteration: 2520  loss: 0.150321\n",
      "lambda1 = 1.0   || lambda_hat_1 = 0.267722\n",
      "lambda2 = 0.01   || lambda_hat_2 = -0.014760\n",
      "iteration: 2540  loss: 0.150312\n",
      "lambda1 = 1.0   || lambda_hat_1 = 0.265752\n",
      "lambda2 = 0.01   || lambda_hat_2 = -0.015177\n",
      "iteration: 2560  loss: 0.150306\n",
      "lambda1 = 1.0   || lambda_hat_1 = 0.266104\n",
      "lambda2 = 0.01   || lambda_hat_2 = -0.014860\n",
      "iteration: 2580  loss: 0.150301\n",
      "lambda1 = 1.0   || lambda_hat_1 = 0.267425\n",
      "lambda2 = 0.01   || lambda_hat_2 = -0.013781\n",
      "iteration: 2600  loss: 0.150295\n",
      "lambda1 = 1.0   || lambda_hat_1 = 0.267375\n",
      "lambda2 = 0.01   || lambda_hat_2 = -0.013730\n",
      "iteration: 2620  loss: 0.150287\n",
      "lambda1 = 1.0   || lambda_hat_1 = 0.267215\n",
      "lambda2 = 0.01   || lambda_hat_2 = -0.013924\n",
      "iteration: 2640  loss: 0.150281\n",
      "lambda1 = 1.0   || lambda_hat_1 = 0.267546\n",
      "lambda2 = 0.01   || lambda_hat_2 = -0.013655\n",
      "iteration: 2660  loss: 0.150276\n",
      "lambda1 = 1.0   || lambda_hat_1 = 0.268819\n",
      "lambda2 = 0.01   || lambda_hat_2 = -0.013818\n",
      "iteration: 2680  loss: 0.150272\n",
      "lambda1 = 1.0   || lambda_hat_1 = 0.270607\n",
      "lambda2 = 0.01   || lambda_hat_2 = -0.013175\n",
      "iteration: 2700  loss: 0.150269\n",
      "lambda1 = 1.0   || lambda_hat_1 = 0.273344\n",
      "lambda2 = 0.01   || lambda_hat_2 = -0.013716\n",
      "iteration: 2720  loss: 0.150265\n",
      "lambda1 = 1.0   || lambda_hat_1 = 0.274994\n",
      "lambda2 = 0.01   || lambda_hat_2 = -0.014329\n",
      "iteration: 2740  loss: 0.150261\n",
      "lambda1 = 1.0   || lambda_hat_1 = 0.276314\n",
      "lambda2 = 0.01   || lambda_hat_2 = -0.013656\n",
      "iteration: 2760  loss: 0.150258\n",
      "lambda1 = 1.0   || lambda_hat_1 = 0.275911\n",
      "lambda2 = 0.01   || lambda_hat_2 = -0.014113\n",
      "iteration: 2780  loss: 0.150254\n",
      "lambda1 = 1.0   || lambda_hat_1 = 0.275713\n",
      "lambda2 = 0.01   || lambda_hat_2 = -0.014603\n",
      "iteration: 2800  loss: 0.150250\n",
      "lambda1 = 1.0   || lambda_hat_1 = 0.273376\n",
      "lambda2 = 0.01   || lambda_hat_2 = -0.013977\n",
      "iteration: 2820  loss: 0.150246\n",
      "lambda1 = 1.0   || lambda_hat_1 = 0.274492\n",
      "lambda2 = 0.01   || lambda_hat_2 = -0.014075\n",
      "iteration: 2840  loss: 0.150243\n",
      "lambda1 = 1.0   || lambda_hat_1 = 0.274233\n",
      "lambda2 = 0.01   || lambda_hat_2 = -0.013874\n",
      "iteration: 2860  loss: 0.150239\n",
      "lambda1 = 1.0   || lambda_hat_1 = 0.272057\n",
      "lambda2 = 0.01   || lambda_hat_2 = -0.013177\n",
      "iteration: 2880  loss: 0.150236\n",
      "lambda1 = 1.0   || lambda_hat_1 = 0.271861\n",
      "lambda2 = 0.01   || lambda_hat_2 = -0.013784\n",
      "iteration: 2900  loss: 0.150233\n",
      "lambda1 = 1.0   || lambda_hat_1 = 0.270612\n",
      "lambda2 = 0.01   || lambda_hat_2 = -0.013623\n",
      "iteration: 2920  loss: 0.150227\n",
      "lambda1 = 1.0   || lambda_hat_1 = 0.268513\n",
      "lambda2 = 0.01   || lambda_hat_2 = -0.012962\n",
      "iteration: 2940  loss: 0.150223\n",
      "lambda1 = 1.0   || lambda_hat_1 = 0.266982\n",
      "lambda2 = 0.01   || lambda_hat_2 = -0.012341\n",
      "iteration: 2960  loss: 0.150218\n",
      "lambda1 = 1.0   || lambda_hat_1 = 0.266208\n",
      "lambda2 = 0.01   || lambda_hat_2 = -0.012719\n",
      "iteration: 2980  loss: 0.150210\n",
      "lambda1 = 1.0   || lambda_hat_1 = 0.265837\n",
      "lambda2 = 0.01   || lambda_hat_2 = -0.013421\n",
      "iteration: 3000  loss: 0.150204\n",
      "lambda1 = 1.0   || lambda_hat_1 = 0.264015\n",
      "lambda2 = 0.01   || lambda_hat_2 = -0.013025\n",
      "iteration: 3020  loss: 0.150200\n",
      "lambda1 = 1.0   || lambda_hat_1 = 0.263121\n",
      "lambda2 = 0.01   || lambda_hat_2 = -0.012657\n",
      "iteration: 3040  loss: 0.150195\n",
      "lambda1 = 1.0   || lambda_hat_1 = 0.262944\n",
      "lambda2 = 0.01   || lambda_hat_2 = -0.013044\n",
      "iteration: 3060  loss: 0.150190\n",
      "lambda1 = 1.0   || lambda_hat_1 = 0.263786\n",
      "lambda2 = 0.01   || lambda_hat_2 = -0.012636\n",
      "iteration: 3080  loss: 0.150184\n",
      "lambda1 = 1.0   || lambda_hat_1 = 0.267412\n",
      "lambda2 = 0.01   || lambda_hat_2 = -0.012575\n",
      "iteration: 3100  loss: 0.150179\n",
      "lambda1 = 1.0   || lambda_hat_1 = 0.268552\n",
      "lambda2 = 0.01   || lambda_hat_2 = -0.011948\n",
      "iteration: 3120  loss: 0.150171\n",
      "lambda1 = 1.0   || lambda_hat_1 = 0.269256\n",
      "lambda2 = 0.01   || lambda_hat_2 = -0.012123\n",
      "iteration: 3140  loss: 0.150168\n",
      "lambda1 = 1.0   || lambda_hat_1 = 0.269759\n",
      "lambda2 = 0.01   || lambda_hat_2 = -0.012202\n",
      "iteration: 3160  loss: 0.150165\n",
      "lambda1 = 1.0   || lambda_hat_1 = 0.270093\n",
      "lambda2 = 0.01   || lambda_hat_2 = -0.011751\n",
      "iteration: 3180  loss: 0.150161\n",
      "lambda1 = 1.0   || lambda_hat_1 = 0.268828\n",
      "lambda2 = 0.01   || lambda_hat_2 = -0.011942\n",
      "iteration: 3200  loss: 0.150159\n",
      "lambda1 = 1.0   || lambda_hat_1 = 0.268371\n",
      "lambda2 = 0.01   || lambda_hat_2 = -0.012288\n"
     ]
    }
   ],
   "source": [
    "#L-BFGS training\n",
    "tr_loss = []\n",
    "tr_lbfgs(model_i, x_train, y_train, t_train, u_train, v_train)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# training loop -- Adam\n",
    "epochs = 10000\n",
    "tr_loss = []\n",
    "for epoch in tqdm(range(epochs)):\n",
    "    l = tr_step(model_i, opt, x_train, y_train, t_train, u_train, v_train) \n",
    "    tr_loss.append(l)\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Training Loss at epoch {epoch + 1}: {tr_loss[-1]:.6f}\")\n",
    "    if epoch % 20 == 0:\n",
    "        print(f\"lambda1 = {1.}     || lambda_hat_1 = {model_i.lambda_1.item():.6f}\")\n",
    "        print(f\"lambda2 = {0.01}   || lambda_hat_2 = {model_i.lambda_2.item():.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = torch.optim.Adam(model_i.parameters(), lr = 1e-4)\n",
    "# training loop -- Adam\n",
    "epochs = 10000\n",
    "tr_loss = []\n",
    "for epoch in tqdm(range(epochs)):\n",
    "    l = tr_step(model_i, opt, x_train, y_train, t_train, u_train, v_train) \n",
    "    tr_loss.append(l)\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Training Loss at epoch {epoch + 1}: {tr_loss[-1]:.6f}\")\n",
    "    if epoch % 20 == 0:\n",
    "        print(f\"lambda1 = {1.}     || lambda_hat_1 = {model_i.lambda_1.item():.6f}\")\n",
    "        print(f\"lambda2 = {0.01}   || lambda_hat_2 = {model_i.lambda_2.item():.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0009819908300414681, -1.0299073665009928e-06)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_i.lambda_1.item(), model_i.lambda_2.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_unique = torch.unique(t_train)\n",
    "t_unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training in a seq2seq approach\n",
    "\n",
    "for i in tqdm(range(t_unique.shape[0])):\n",
    "    iter = 0\n",
    "    tr_loss = []\n",
    "    print(f\"Training for snapshot t = {t_unique[i].item():.3f}\")\n",
    "    x_new, y_new, t_new, u_new, v_new = x_train[t_train == t_unique[i]], y_train[t_train == t_unique[i]],\\\n",
    "                                        t_train[t_train == t_unique[i]], u_train[t_train == t_unique[i]],\\\n",
    "                                        v_train[t_train == t_unique[i]]\n",
    "    tr_lbfgs(model_i, x_new, y_new, t_new, u_new, v_new)\n",
    "    print(f\"Training snapshot t = {t_unique[i].item():.3f} finished.\")\n",
    "    plt.figure()\n",
    "    plt.plot(tr_loss)\n",
    "    plt.xlabel(\"#trainin-steps\")\n",
    "    plt.title(f\"Traning Loss at snapshot t={t_unique[i].item():.3f}\")\n",
    "    plt.savefig(f\"plots_training/loss_snap_t_{t_unique[i].item():.3f}.png\")\n",
    "    plt.close()\n",
    "\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Parameter containing:\n",
       " tensor([-0.0092], device='cuda:0', requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([0.0237], device='cuda:0', requires_grad=True))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_i.lambda_1, model_i.lambda_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
